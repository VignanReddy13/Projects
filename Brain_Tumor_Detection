# ==========================================
# ðŸ§  Brain Tumor Detection - FIXED VERSION
# ==========================================

# 1ï¸âƒ£ Upload and Extract
from google.colab import files
uploaded = files.upload()

import zipfile
import os
import shutil

zip_file_name = list(uploaded.keys())[0]

with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:
    zip_ref.extractall("/content/dataset")

print("Extraction Completed!")

# 2ï¸âƒ£ Auto Fix Folder Structure

BASE_PATH = "/content/dataset"
folders = os.listdir(BASE_PATH)

# If unwanted nested folder exists, go inside it
if "brain_tumor_dataset" in folders:
    DATASET_PATH = os.path.join(BASE_PATH, "brain_tumor_dataset")
else:
    DATASET_PATH = BASE_PATH

print("Using dataset path:", DATASET_PATH)
print("Classes found:", os.listdir(DATASET_PATH))

# 3ï¸âƒ£ Imports
import cv2
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping

# 4ï¸âƒ£ Parameters
IMG_SIZE = 128
BATCH_SIZE = 32
EPOCHS = 5

# 5ï¸âƒ£ Data Generator
datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2
)

train_data = datagen.flow_from_directory(
    DATASET_PATH,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='binary',
    subset='training'
)

val_data = datagen.flow_from_directory(
    DATASET_PATH,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='binary',
    subset='validation'
)

# 6ï¸âƒ£ Build Model (Proper Input Layer)

model = Sequential([
    Input(shape=(IMG_SIZE, IMG_SIZE, 3)),
    Conv2D(32, (3,3), activation='relu'),
    MaxPooling2D(2,2),

    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),

    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D(2,2),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model.summary()

# 7ï¸âƒ£ Train
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True
)

history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=EPOCHS,
    callbacks=[early_stop]
)

# 8ï¸âƒ£ Evaluation
val_data.reset()
predictions = model.predict(val_data)
predictions = (predictions > 0.5).astype(int)

y_true = val_data.classes

print("\nClassification Report:")
print(classification_report(y_true, predictions))

cm = confusion_matrix(y_true, predictions)

plt.figure()
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

print("Training Complete âœ…")
